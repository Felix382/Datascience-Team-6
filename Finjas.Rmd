---
title: "R Notebook"
output: html_notebook
---
---
title: "R Notebook"
output: html_notebook
---


```{r}
library(readr)
library(lubridate)
library(ggplot2)
library(dplyr)
library(leaps)
library (pracma)
library(readxl)

umsatzdaten <- read_csv("data/umsatzdaten_gekuerzt.csv")
kiwo<- read_csv("data/kiwo.csv")
feiertage <- read_csv("data/feiertage.csv")
Zeiten <- read_excel("data/Zeiten.xlsx")
feiertage <- read_csv("data/feiertage.csv")
wetter <- read_csv("data/wetter.csv")
niederschlagsmenge <- read_csv("data/Niederschlagsmenge.csv")
sonnenschein <- read_csv("data/Sonnenscheindauer.csv")

merger<-left_join(umsatzdaten,kiwo)
Zeiten <- Zeiten %>% 
mutate(Datum = as.Date(Datum)) 
merger <- merger %>% 
mutate(Datum = as.Date(Datum)) 
merger2<-left_join(merger,Zeiten, by='Datum')
merger2[is.na(merger2)]<-0
merger2$wochentag <- weekdays(merger2$Datum)
merger3<-left_join(merger2,feiertage)
merger3[is.na(merger3)]<-0
merger4<-left_join(merger3, niederschlagsmenge, by='Datum')
merger5<-left_join(merger4, sonnenschein, by='Datum')


 
final<-left_join (merger5, wetter)

#merger5<-left_join(merger4,)
#final<-left_join(merger4, merger1_2)
View(final)

#write.csv2(final, "dateiname.csv")
```



```{r}
#Dummy für Wochentage, Stena Line fährt jeden Tag

final%>%
mutate(Dummy = ifelse(wochentag=='Dienstag', "1", "0"))

final%>%
lag(feiertage, k=1)
  

movag<-movavg(wetter$Temperatur, 7, type=c("s"))


####UNDER CONSTRUCTION####
#Moving Average
# Function ma() calculates the moving average of order n
ma <- function(x, n = 7) { filter(x, rep(1/n,n), sides = 7, method = "convolution") }

for (Datum in final) {
  
}

movingaverage <- data.frame(final$Datum, "Moving Average")
# Calculate moving averages of order 7
ma7 <- ma(final$Umsatz, 7)

```

Balkendiagramm Beispiel 
```{r}
wetter <- read_csv("data/wetter.csv")
sonne <- read_csv("data/Sonnenscheindauer.csv")

View (wetter)
View(sonne)

```



```{r}
wetter$month<-month(as.POSIXlt(wetter$Datum, format="%d/%m/%Y"))

wetter$Monate<-month.abb[wetter$month]

wetter$Monate <- factor(wetter$Monate, levels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
View (wetter)

```



```{r}
sonne$month<-month(as.POSIXlt(sonne$Datum, format="%d/%m/%Y"))

sonne$Monate<-month.abb[sonne$month]

sonne$Monate <- factor(sonne$Monate, levels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
View (sonne)

```


```{r}
my_sum <- wetter %>%
  group_by(Monate) %>%
  summarise(
    n=n(),
    mean=mean(Temperatur),
    sd=sd(Temperatur)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1))

# Confidence Interval
ggplot(my_sum) +
  geom_bar( aes(x=Monate, y=mean), stat="identity", fill="forestgreen", alpha=0.5) +
  geom_errorbar( aes(x=Monate, ymin=mean-ic, ymax=mean+ic), width=0.4, colour="orange", alpha=0.9, size=1.5) +
  ggtitle("Durchschnittliche Monatstemperatur")

```


```{r}
my_sum <- sonne %>%
  group_by(Monate) %>%
  summarise(
    n=n(),
    mean=mean(Sonnenscheindauer),
    sd=sd(Sonnenscheindauer)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1))

# Confidence Interval
ggplot(my_sum) +
  geom_bar( aes(x=Monate, y=mean), stat="identity", fill="forestgreen", alpha=0.5) +
  geom_errorbar( aes(x=Monate, ymin=mean-ic, ymax=mean+ic), width=0.4, colour="orange", alpha=0.9, size=1.5) +
  ggtitle("Durchschnittliche Sonnenscheindauer")

```



Sollten einen Dummy für vor oder nach Feiertag nehmen, Tage vor dem eigentlichen verkauf mit reinnehmen

Lineare Regression

```{r}
mod<- lm(Umsatz~KielerWoche+ Ferien+ wochentag + Feiertag + Temperatur+as.factor(Warengruppe)+ Bewoelkung+ Vor+ Nach,final)
summary(mod)

```



Subsets
```{r}
y <- regsubsets(Umsatz ~ ., final, nvmax=10)
summary(y)

```

### # Installation ggf. noch benötigter Pakete ###
```{r}
# Nur ausführen, beim allerersten Mal !!
install.packages("fastDummies")
install.packages("reticulate")
install.packages("Metrics")

library(reticulate)
py_install("pandas")
py_install("numpy")
py_install("tensorflow")

```

###Vorbereitung der Umgebung
```{r}

# Einbinden benötogter Funktionsbibliotheken
library(reticulate)
library(readr)
library(fastDummies)
library(ggplot2)
library(Metrics)


# Funktionsdefinitionen

#' Title Fast creation of normalized variables
#' Quickly create normalized columns from numeric type columns in the inputted data. This function is useful for statistical analysis when you want normalized columns rather than the actual columns.
#'
#' @param .data An object with the data set you want to make normalized columns from.
#' @param norm_values Dataframe of column names, means, and standard deviations that is used to create corresponding normalized variables from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) with same number of rows as inputted data and original columns plus the newly created normalized. columns.
#' @export
#'
#' @examples
norm_cols <- function (.data, norm_values = NULL) {
  for (i in 1:nrow(norm_values)  ) {
    .data$norm <- (.data[[norm_values$name[i]]] - norm_values$mean[i]) / norm_values$sd[i]
    names(.data)[length(.data)] <- paste0(norm_values$name[i], "_norm")
  }
  return (.data)
}


#' Title Creation of a Dataframe including the Information to Standardize Variables
#' This function is meant to be used in combination with the function norm_cols
#'
#' @param .data A data set including the variables you want to get the means and standard deviations from.
#' @param select_columns A vector with a list of variable names for which you want to get the means and standard deviations from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) including the names, means, and standard deviations of the variables included in the select_columns argument.
#' @export
#'
#' @examples
get.norm_values <- function (.data, select_columns = NULL) {
  result <- NULL
  for (col_name in select_columns) {
    mean <- mean(.data[[col_name]])
    sd <- sd(.data[[col_name]] na.rm = TRUE)
    result <- rbind (result, c(mean, sd))
  }
  result <- as.data.frame(result, stringsAsFactors = FALSE)
  result <- data.frame (select_columns, result, stringsAsFactors = FALSE)
  names(result) <- c("name", "mean", "sd")
  return (result)
}
```



```{r}
# Rekodierung von kategoriellen Variablen (zu Dummy-Variablen)
dummy_list <- c("Warengruppe", "wochentag", "Bewoelkung")
final_dummy = dummy_cols(final, dummy_list)

# Standardisierung von metrischen Variablen
norm_list <- c('Umsatz', 'Temperatur', 'Sonnenscheindauer', 'Niederschlagsmenge', 'KielerWoche')
# Berechnung der Mittelwerte und Standardabweichungen der zu standardisierenden Variablen
norm_values_list <- get.norm_values(final_dummy, norm_list)
# Standardisierung der angegebenen metrischen Variablen
final_norm <- norm_cols(final_dummy, norm_values_list)

# Definition von Variablenlisten, um das Arbeiten mit diesen zu erleichtern
warengruppen_dummies = c('Warengruppe_1', 'Warengruppe_2','Warengruppe_3','Warengruppe_4','Warengruppe_5','Warengruppe_6')
wochentage_dummies = c('wochentag_Sonntag','wochentag_Montag', 'wochentag_Dienstag', 'wochentag_Mittwoch','wochentag_Donnerstag','wochentag_Freitag','wochentag_Samstag')
Bewoelkung_dummies = c('Bewoelkung_0','Bewoelkung_1','Bewoelkung_2','Bewoelkung_3','Bewoelkung_4','Bewoelkung_5','Bewoelkung_6','Bewoelkung_7','Bewoelkung_8')

# Definition der Features (der unabhängigen Variablen auf deren Basis die Vorhersagen erzeugt werden sollen)
features = c('Temperatur_norm', 'Sonnenscheindauer_norm', 'Niederschlagsmenge_norm', 'KielerWoche', 'Ferien', 'Nach', 'Vor', 'Feiertag', warengruppen_dummies, wochentage_dummies, Bewoelkung_dummies)
# Definition der Label-Variable (der abhaengigen Variable, die vorhergesagt werden soll) sowie
label = 'Umsatz_norm'


# Zufallszähler setzen, um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten
set.seed(1)
# Bestimmung der Indizes des Traininsdatensatzes
train_ind <- sample(seq_len(nrow(final_norm)), size = floor(0.80 * nrow(final_norm)))

# Teilen in Trainings- und Testdatensatz
train_dataset = final_norm[train_ind, features]
test_dataset = final_norm[-train_ind, features]

# Selektion der Variable, die als Label definiert wurde
train_labels = final_norm[train_ind, label]
test_labels = final_norm[-train_ind, label]
```


### Schätzung des Neuronalen Netzes
```{python}
# Benoetigte Python Libraries einbinden
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Definition der Form des tiefen neuronalen Netzes (Deep Neural Nets)
model = keras.Sequential([
  layers.Dense(5, activation='relu', input_shape=[len(r.train_dataset.keys())]),
  layers.Dense(4, activation='relu'),
  layers.Dense(1)
])

# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model.compile(loss="mse",
              optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))

# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model.summary()

# Schaetzung des Modells
history = model.fit(r.train_dataset, r.train_labels, epochs=150, validation_split = 0.1, verbose=0)

```



### Auswertung der Modelloptimierung ###
```{r}
# Grafische Ausgabe der Modelloptimierung

# create data
data <- data.frame(val_loss = unlist(py$history$history$val_loss),
                  loss = unlist(py$history$history$loss))

# Plot
ggplot(data[-1,]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") 


```





### Auswertung der Schätzergebnisse ###
```{r}
# Schätzung der (normierten) Preise für die Trainings- und Testdaten
train_predictions_norm <- py$model$predict(train_dataset)
test_predictions_norm <- py$model$predict(test_dataset)

# Rückberechnung der normierten Preisschätzungen zu den tatsächlichen Preisschätzungen bzw. Preisen
train_predictions <- (train_predictions_norm * norm_values_list$sd[1] ) + norm_values_list$mean[1]
test_predictions <- (test_predictions_norm * norm_values_list$sd[1]) + norm_values_list$mean[1]
# Selektion der zugehörigen tatsächlichen Preise
train_actuals <- final$Umsatz[train_ind]
test_actuals <- final$Umsatz[-train_ind]


# Vergleich der Gütekriterien für die Traingings- und Testdaten
cat(paste0("MAPE on the Training Data:\t", format(mape(train_actuals, train_predictions)*100, digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Test Data:\t\t", format(mape(test_actuals, test_predictions)*100, digits=3, nsmall=2)))


```

```{r}

## Grafischer vergleich der vorhergesagten und der tatsächlichen Preise für die Trainings- und Testdaten

# Zusammenstellung der Daten für die Plots
data_train <- data.frame(prediction = train_predictions/1000, actual = train_actuals/1000)
data_test <- data.frame(prediction = test_predictions/1000, actual = test_actuals/1000)

# Plot der Ergebnisse der Trainingsdaten
ggplot(data_train[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("Case Number") +
  ylab("Price in 1.000 USD") 

# Plot der Ergebnisse der Testdaten
ggplot(data_test[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("Case Number") +
  ylab("Price in 1.000 USD") 


```

```{r}
# Vorhersage für einen einzelnen Fall
cat(paste0("Vorergesagter Preis:\t", format(test_predictions[100], digits=2, nsmall =0)))
cat(paste0("\nTatsächlicher Preis:\t", test_actuals[100]))


```